Something you should know about Random Forest

1. Brief Introduction
"Random forests are an ensemble learning method for classification (and regression) that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees." -- http://en.wikipedia.org/wiki/Random_forest

2. Objects
(1) Supervised learning.
(2) It is better to think of random forests as a framework rather than as a particular model. 

The framework consists of several interchangeable parts which can be mixed and matched to create a large number of particular models, all built around the same central theme. Constructing a model in this framework requires making several choices:
i). The shape of the decision to use in each node.
ii). The type of predictor to use in each leaf.
iii). The splitting objective to optimize in each node.
iv). The method for injecting randomness into the trees.

The simplest type of decision to make at each node is to apply a threshold to a single dimension of the input. This is a very common choice and leads to trees that partition the space into hyper-rectangular regions. However, other decision shapes, such as splitting a node using linear or quadratic decisions are also possible.

Leaf predictors determine how a prediction is made for a point, given that it falls in a particular cell of the space partition defined by the tree. Simple and common choices here include using a histogram for real valued outputs, or constant predictors for categorical data.

In principle there is no restriction on the type of predictor that can be used, for example one could fit a Support Vector Machine or a spline in each leaf; however, in practice this is uncommon. If the tree is large then each leaf may have very few points making it difficult to fit complex models; also, the tree growing procedure itself may be complicated if it is difficult to compute the splitting objective based on a complex leaf model. However, many of the more exotic generalizations of random forests, e.g. to density or manifold estimation, rely on replacing the constant leaf model.

The splitting objective is a function which is used to rank candidate splits of a leaf as the tree is being grown. This is commonly based on an impurity measure, such as the information gain or the Gini gain.

The method for injecting randomness into each tree is the component of the random forests framework which affords the most freedom to model designers. Breiman's original algorithm achieves this in two ways:
Each tree is trained on a bootstrapped sample of the original data set.
Each time a leaf is split, only a randomly chosen subset of the dimensions are considered for splitting.

3. Algorithm
----------------- Breiman's Algorithm -------------------
Each tree is constructed using the following algorithm:

(1) Let the number of training cases be N, and the number of variables in the classifier be M.
(2) We are told the number m of input variables to be used to determine the decision at a node of the tree; m should be much less than M.
(3) Choose a training set for this tree by choosing n times with replacement from all N available training cases (i.e., take a bootstrap sample). Use the rest of the cases to estimate the error of the tree, by predicting their classes.
(4) For each node of the tree, randomly choose m (out of M) variables on which to search for the best split. Calculate the best split based on these m variables in the training set. Base the decision at that node using the best split.
(5) Each tree is fully grown and not pruned (as may be done in constructing a normal tree classifier).

For prediction a new sample is pushed down the tree. It is assigned the label of the training sample in the terminal node it ends up in. This procedure is iterated over all trees in the ensemble, and the mode vote of all trees is reported as the random forest prediction.

4. Advantages
(1) Random Forests are a wonderful tool for making predictions considering they do not overfit because of the law of large numbers. Introducing the right kind of randomness makes them accurate classifiers and regressors.
(2) Single decision trees often have high variance or high bias. Random Forests attempts to mitigate the problems of high variance and high bias by averaging to find a natural balance between the two extremes.
(3) Considering that Random Forests have few parameters to tune and can be used simply with default parameter settings, they are a simple tool to use without having a model or to produce a reasonable model fast and efficiently.
(4) Random Forests are easy to learn and use for both professionals and lay people - with little research and programming required and may be used by folks without a strong statistical background. Simply put, you can safely make more accurate predictions without most basic mistakes common to other methods (They are able to deal with unbalanced and missing data).
(5) Accuracy.
(6) Runs efficiently on large data bases.  
(7) Handles thousands of input variables without variable deletion.
(8) Gives estimates of what variables are important in the classification.
(9) Generates an internal unbiased estimate of the generalization error as the forest building progresses.
(10) Provides effective methods for estimating missing data.
(11) Maintains accuracy when a large proportion of the data are missing.
(12) Provides methods for balancing error in class population unbalanced data sets.
(13) Generated forests can be saved for future use on other data.
(14) Prototypes are computed that give information about the relation between the variables and the classification.
(15) Computes proximities between pairs of cases that can be used in clustering, locating outliers, or (by scaling) give interesting views of the data.
(16) Capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection.
(17) Offers an experimental method for detecting variable interactions.

5. Disadvantages
(1) With a large number of predictors, the eligible predictor set will be quite different from node to node.
(2) The greater the inter-tree correlation, the greater the random forest error rate, so one pressure on the model is to have the trees as uncorrelated as possible. (Solution: As m goes down, both inter-tree correlation and the strength of individual trees go down. So some optimal value of m must be discovered.)
(3) When used for regression they cannot predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy. Of course, the best test of any algorithm is how well it works upon your own data set.

6. Improvement/Variants
Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers.

7. Explanation in layman term:
http://qr.ae/hf1Jp

8. The original paper is from Leo Breiman:
http://scholar.google.com/citations?view_op=view_citation&hl=en&user=mXSv_1UAAAAJ&citation_for_view=mXSv_1UAAAAJ:d1gkVwhDpl0C

9. Other sources:
http://www.whrc.org/education/indonesia/pdf/DecisionTrees_RandomForest_v2.pdf

http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm
